\documentclass[xcolor=pdftex,dvipsnames,usenames,table]{beamer}

\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
%\usepackage{times}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsmath}

\mode<presentation>
{
  \usetheme{Singapore}
  \usecolortheme{default}
  \setbeamercovered{transparent}

  %\setbeamertemplate{part page}
  %{
    %\begin{centering}
      %\begin{beamercolorbox}[sep=8pt,center]{part title}
        %\usebeamerfont{part title}\insertpart\par
      %\end{beamercolorbox}
    %\end{centering}
  %}
}

\title[]
{Primitivas de Memoria Virtual para Porgramas de Usuario}

\subtitle
{Virtual Memory Primitives for User Programs}

\author[]
{
	Leandro Liptak\\
	Patricio Reboratti\\
	Damián Silvani
}

\institute[UBA]
{
  Programación de Sistemas Operativos\\
  Universidad de Buenos Aires
}
\date
{9 de junio, 2011}

% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:
\beamerdefaultoverlayspecification{<+->}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\section{Introducción}

\begin{frame}{Memoria Virtual}
  \begin{itemize}
    \item Principalmente, permite
      \begin{itemize}
        \item Extender el espacio de direccionamiento
        \item Compartir páginas entre procesos
        \item Proteger páginas de código como \textit{sólo lectura}
        \item Implementar \texttt{copy-on-write}
      \end{itemize}

    \item Puede tener otras aplicaciones\ldots
    \begin{itemize}
      \item Unix traduce un PF en un SIGSEGV, el usuario puede atraparlo
      \item Se puede hacer algo mejor?
    \end{itemize}
 \end{itemize}
\end{frame}

\begin{frame}{Primitivas de Memoria Virtual}
  \begin{itemize}
    \item \texttt{trap}
    \item \texttt{prot1} y \texttt{protN}
    \item \texttt{unprot}
    \item \texttt{dirty}
    \item \texttt{map2}
  \end{itemize}
\end{frame}

\section{Aplicaciones}

\begin{frame}{Memoria virtual compartida}
  \begin{itemize}
    \item Memoria compartida, distribuida en varias computadoras
    \item Acceso a una memoria compartida coherente
    \item Se divide el espacio en páginas:
    \begin{itemize}
      \item En escritura, si la página reside en otras memorias físicas, se
            busca una copia actualizada y se invalida las otras copias.
      \item Las de sólo lectura pueden estar en varias memorias físicas al mismo
            tiempo.
    \end{itemize}
    \item Funciona similar a la virtualización con swapping:
    \begin{itemize}
      \item Un Page Fault ocurre cuando la página no se encuentra en la propia
            memoria física.
      \item El MMU puede buscar la página en el disco o en otra memoria física.
    \end{itemize}
    \item Necesita \texttt{trap}, \texttt{prot1}, \texttt{unprot} y \texttt{map2}
  \end{itemize}
\end{frame}

\begin{frame}{Checkpointing concurrente}
\end{frame}

\begin{frame}{GC concurrente}
\pause En un Garbage Collector incremental basado en copia como el Baker's GC, la protección de páginas y el manejo de fallos por parte del recolector brinda un mecanismo eficiente de sincronización entre los treads mutadores y el recolector.

\pause Para esto (suponiento un solo thread mutador):
	\pause
	\begin{itemize}
		\item Se crean dos mapeos de las páginas de \emph{from-space} con \textbf{\texttt{map2}}
		\item Para cada página, se protege el mapeo que corresponde al thread del mutador con \textbf{\texttt{protN}}
	\end{itemize}

\pause Ante una referencia a un objeto en \emph{from-space} por parte del mutador, el thread recolector:
	\pause
	\begin{itemize}
		\item Atrapa el fallo con \textbf{\texttt{trap}}
		\item Copia los objetos "vivos" de dicha página a \emph{to-space}
		\item Actualiza los punteros a dichos objetos en el thread mutador
		\item Desprotege la páginas en cuestión con \textbf{\texttt{unprot}}
	\end{itemize}

\end{frame}

\begin{frame}{GC generacional}
\end{frame}

\begin{frame}{Heap persistente}
\end{frame}

\begin{frame}{Extensión de direccionamiento}
\end{frame}

\begin{frame}{Compresión de páginas}
\pause En una típica estructura de datos vinculada, en general encontramos:
	\pause
	\begin{itemize}
		\item Punteros que apuntan a direcciones de memoria cercanas.
		\item Punteros nulos.
		\item Enteros pequeños.
		\item Enteros en cero.
	\end{itemize}

\pause En términos de teoría de la información, la entropía de la palabra promedio es pequeña.

\pause Implementando un GC para hacer que los punteros que se referencian entre sí estén en direcciones cercanas, se puede achicar la entropía por palabra hasta un valor tan chico como 7 bits.

\pause De esta manera, una página de palabras de 32 bits puede comprimirse a aproximadamente un cuarto de su tamaño. De este modo en vez de guardar las páginas menos recientemente usadas en el disco, se podría comprimir y almacenar en otro lugar de la memoria principal.

\pause Compresión en OS vs. Compresión controlada por el usuario.

\end{frame}

\begin{frame}{Detección de heap overflow}
\end{frame}

\section{Consideraciones}

\begin{frame}{Consistencia de la TLB}
  \begin{itemize}
    \item Cuando una página se hace más accesible, una TLB desactualizada a lo
          sumo causa un \textit{cache miss}.
    \item Pero al proteger una página, es necesario invalidar la TLB.
    \item En multiprocesadores, el problema es más notorio:
    \begin{itemize}
      \item Se debe interrumpir cada procesador e invalidar sus TLBs.
      \item Costoso por software
    \end{itemize}
    \item Solución: hacer \textit{flush} por lote
    \item La mayoria de los algoritmos usan \texttt{protN}, no \texttt{prot1}
  \end{itemize}
\end{frame}

\begin{frame}{Tamaño de página óptimo}

\pause Cuando un page-fault ocurre y hay que hacer un swap entre memoria y disco, el overhead en el handler del page-fault es ínfimo en comparación con el tiempo que tomará el acceso a memoria y disco. Por este motivo, las páginas en general son bastante grandes y el overhead en los handlers es alto.

\pause La mayor parte de los algoritmos presentados en este paper utilizan page-faults que son completamente atendidos por el CPU. Entonces se vuelve importante reducir al mínimo el overhead en el handler.

\pause Trabajado en el CPU, el tiempo que toma atender el fallo se puede calcular con una constante pequeña por el tamaño de la página. Por lo tanto, con páginas pequeñas el tiempo es menor. Pero cuanto más pequeña la página, más se nota el overhead (que es independiente del tamaño).

\pause Para \texttt{prot} y \texttt{unprot} es recomendable usar páginas pequeñas. Para paginar a disco, es recomendable usar bloques de varias páginas contiguas.

\end{frame}

\begin{frame}{Acceso a páginas protegidas}
\pause ¿Cómo lograr el acceso a páginas protegidas sólo por parte de algunos threads?

\pause
\begin{itemize}
	\item Proveyendo (SO) \texttt{map2} y utilizando \texttt{prot1} o \texttt{protN}
	\item Proveyendo (SO) una \emph{syscall} para copiar desde/hacia una página protegida
	\item Utilizando procesos en lugar de threads si el SO soporta compartición de memoria entre éstos

\end{itemize}

\end{frame}

\begin{frame}{Trap handlers sincrónicos}
\end{frame}

\section{Conclusiones}

\begin{frame}{Conclusión}
\end{frame}

\end{document}
